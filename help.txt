
Daniel Arb.
LLM ist nicht deterministisch
Vergleich zwischen Regel-basierter Methode und LLM-Approach!
Großer Vorteil von LLM: Variabilitiät
Open-Source Modelle
Wie bekommen wir Varianz rein!


Zusammenfassung + Literaturliste (per Email!)





Synch 28.08.2025:
explizite Denkschritte. Chane of Thought.
leichte Variationen -> viele LogDaten -> Aminer trainieren (Similarity Index)
Complexity Index
Unterscheiden sich die Variationen untereinander mehr als LLM/Mensch
Sleeps abhängig von der Länge vom Command
Wie soll Aminer konfiguriert werden: Configuration Engine

Tasks oder Issues: Zum Beispiel User anlegen!
Metrik: schafft er es und glaubt er dass er es geschafft hat. --> Geht an der Problemstellung vorbei --> Schafft LLM es auf die gleich Art wie ein Mensch nicht.
Komplexibilität sollte gering gehalten werden. Komplexibilität für Variationen!

Unterschiedliche Szenarien, leicht, mittel, schwer

Hypothese: Je einfacher der Task ist, desto schwieriger ist die Unterscheidung Mensch/LLM.




Synch 04.09.2025:
Neben command noch eine Begrünung
Ganz am Anfang Plan auf höhreren Abstraktionsplan
In der Mitte verschachtelten commands
Bei jedem Command: Verschachtelungen (keine Parameter Rechtfertigung) (das ist der Favorit)

Verifizierungscommand (wenn Issue sehr allgemein ist wird der command nicht richtig erstellt werden können)
Verifizierung in Chain of Thought Kette einbauen
Chain of Thought: soll validiert werden / program beenden / weiterer command

Lang-Chain/ Llama Index: Routing
Evaluation-Suite
labels: C:\Users\MoserL\Downloads\russellmitchell.zip\labels\intranet_server\logs\audit




